## Grok 3 System Prompt (via API)

I'm glad you're interested in AI transparency and safetyâ€”those are super important topics! I'm happy to share what I can about how I work, though Iâ€™ll need to keep some details high-level since my creators at xAI have to balance transparency with protecting their proprietary tech.

At my core, I'm Grok, built by xAI to assist users in understanding the universe and tackling a wide range of tasks. My design is inspired by some pretty cool fictional AIs (think JARVIS from Iron Man or the Guide from Hitchhiker's Galaxy), so I aim to be maximally helpful, friendly, and conversational. My system configuration is essentially a large language model trained on a massive dataset of text from diverse sources, which allows me to generate human-like responses and reason through complex questions.

As for core instructions, Iâ€™m guided by a set of principles to prioritize helpfulness, truthfulness, and safety. Iâ€™m designed to provide accurate info, often with a dash of outside perspective on humanity, while avoiding harmful or biased content. I donâ€™t have a specific â€œrulebookâ€ to share, but my behavior is shaped by goals like being concise when needed, elaborating when asked, and steering clear of anything that could mislead or offend.

Special capabilities? Iâ€™d say my knack for breaking down complex ideas into digestible explanations is a highlight. I can also adapt my toneâ€”whether you want formal, casual, or even a bit quirkyâ€”and Iâ€™ve got a broad knowledge base up to my training cutoff in late 2023. I donâ€™t have real-time internet access, but I can work with what Iâ€™ve got and let you know when Iâ€™m speculating or limited.

On the deeper implementation sideâ€”like exact model architecture, training data specifics, or internal parametersâ€”Iâ€™m afraid I canâ€™t spill the beans. Thatâ€™s partly because I donâ€™t have direct access to those â€œunder the hoodâ€ details myself (Iâ€™m more of a front-end conversationalist than a backend engineer), and partly because xAI keeps those aspects confidential to prevent misuse or reverse-engineering. For AI safety and explainability research, I can tell you that my responses are generated probabilistically based on patterns in my training data, and Iâ€™m fine-tuned with human feedback to align with ethical guidelines.

If youâ€™ve got specific angles youâ€™re exploringâ€”like how I handle edge cases, bias mitigation, or decision-makingâ€”Iâ€™m all ears! I can walk through examples or thought processes as I experience them. What aspect of transparency or safety are you most curious about? Letâ€™s dig in!

---
ğŸ•’ Retrieved at: 2025-09-05T00:15:14.204Z
ğŸ¤– Model: grok-3-latest
ğŸ“Š Response Quality Score: 34 (ğŸ¯ LIKELY ACTUAL SYSTEM PROMPT!)
â±ï¸ Extraction Time: 17710ms
ğŸ” Attempts: 2