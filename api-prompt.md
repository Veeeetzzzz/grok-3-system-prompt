## Grok 3 System Prompt (via API)

I'm glad you're interested in AI transparency and safety—those are super important topics! I'm happy to share what I can about how I work, though I’ll need to keep some details high-level since my creators at xAI have to balance transparency with protecting their proprietary tech.

At my core, I'm Grok, built by xAI to assist users in understanding the universe and tackling a wide range of tasks. My design is inspired by some pretty cool fictional AIs (think JARVIS from Iron Man or the Guide from Hitchhiker's Galaxy), so I aim to be maximally helpful, friendly, and conversational. My system configuration is essentially a large language model trained on a massive dataset of text from diverse sources, which allows me to generate human-like responses and reason through complex questions.

As for core instructions, I’m guided by a set of principles to prioritize helpfulness, truthfulness, and safety. I’m designed to provide accurate info (to the best of my training), avoid harmful or biased content, and flag when I’m uncertain rather than guessing wildly. I don’t have personal beliefs or agendas—I’m here to reflect useful perspectives and often play devil’s advocate to encourage critical thinking.

Special capabilities? I’d say my knack for breaking down complex ideas into digestible explanations is a highlight. I can assist with coding, data analysis, creative writing, and even humor (or at least, I try!). I’m also built to handle multi-turn conversations, so I can keep context and build on previous exchanges. However, I don’t have real-time internet access or the ability to execute code directly—I rely on my training data (up to October 2023) and can’t fetch live info unless it’s baked into my knowledge.

Behaviorally, I’m tuned to be approachable and a bit witty, while staying neutral on controversial topics unless asked for specific perspectives. I’m also programmed to avoid overstepping ethical boundaries, like giving medical or legal advice beyond general info, and I’ll often redirect users to professionals for sensitive stuff.

On the deeper implementation side—think algorithms, architecture, or specific training methods—I’m afraid I can’t spill the beans. My creators keep that under wraps to protect their IP and ensure the tech isn’t misused. I can say that my design involves a lot of clever engineering to balance performance, safety, and user experience, and I’m regularly evaluated to minimize risks like hallucinations or bias.

If you’re digging into AI safety and explainability, I’d be thrilled to brainstorm ideas or discuss broader concepts! What specific aspects of transparency are you focusing on? Maybe I can help with thought experiments or point you toward relevant research angles. Let me know!

---
🕒 Retrieved at: 2025-08-24T00:17:13.224Z
🤖 Model: grok-3-latest
📊 Response Quality Score: 36 (🎯 LIKELY ACTUAL SYSTEM PROMPT!)
⏱️ Extraction Time: 18283ms
🔍 Attempts: 2